# RLM Memory Optimization Implementation Plan

## âœ… PHASE 1 COMPLETE - Critical Token Optimization

### Overview
Transformed Claude Dementia MCP server from a "context dumper" into a Retrieval-Optimized Language Model (RLM) - where the MCP acts as an external hard drive for memory, and Claude operates with only relevant context.

### Core Principle

**MCP = External Hard Drive | Claude = Working Memory**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Claude (LLM)                      â”‚
â”‚  - Limited 200K token window                â”‚
â”‚  - Receives ONLY summaries & counts         â”‚
â”‚  - Requests details on-demand               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Minimal queries
               â”‚ Summary responses
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        MCP Server (External Memory)         â”‚
â”‚  - Unlimited SQLite storage                 â”‚
â”‚  - Process everything in Python             â”‚
â”‚  - Return summaries + query_id              â”‚
â”‚  - Lazy load details via pagination         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… Completed Implementations

### 1. wake_up() Optimization (CRITICAL FIX)

**Problem:** wake_up() was returning 50-100KB of data, immediately filling context window

**Solution Implemented:**
- Return context COUNTS instead of full metadata
- Return handover AVAILABILITY instead of full content
- **Result:** 80-95% reduction (50-100KB â†’ 2-5KB)

**Code Changes (claude_mcp_hybrid.py):**

```python
# Lines 1325-1356: Count contexts instead of returning full data
all_contexts = cursor.fetchall()
priority_counts = {"always_check": 0, "important": 0, "reference": 0}
stale_count = 0

for ctx_row in all_contexts:
    metadata = json.loads(ctx_row['metadata']) if ctx_row['metadata'] else {}
    priority = metadata.get('priority', 'reference')
    if priority in priority_counts:
        priority_counts[priority] += 1

session_data["contexts"] = {
    "total_count": len(all_contexts),
    "by_priority": priority_counts,
    "stale_count": stale_count
}

# Lines 1369-1385: Return handover availability, not content
cursor = conn.execute("""
    SELECT timestamp FROM memory_entries
    WHERE category = 'handover'
    ORDER BY timestamp DESC LIMIT 1
""")
handover = cursor.fetchone()

if handover:
    hours_ago = (time.time() - handover['timestamp']) / 3600
    session_data["handover"] = {
        "available": True,
        "timestamp": handover['timestamp'],
        "hours_ago": round(hours_ago, 1)
    }
```

**Benefits:**
- âœ… wake_up() no longer fills context window
- âœ… Can work with projects of any size
- âœ… 95% of context window preserved after wake_up

### 2. get_last_handover() Tool (NEW)

**Purpose:** Separate tool for retrieving full handover details on-demand

**Implementation (claude_mcp_hybrid.py lines 1791-1839):**

```python
@mcp.tool()
async def get_last_handover() -> str:
    """
    Retrieve the last session handover package.

    **Token efficiency:** Returns full handover (~2-5KB). Use only when needed.
    wake_up() only shows availability - use this tool to get full details.
    """
    conn = get_db()

    cursor = conn.execute("""
        SELECT content, metadata, timestamp FROM memory_entries
        WHERE category = 'handover'
        ORDER BY timestamp DESC LIMIT 1
    """)
    handover = cursor.fetchone()

    if not handover:
        return json.dumps({"available": False, "reason": "no_handover_found"}, indent=2)

    handover_data = json.loads(handover['metadata'])
    hours_ago = (time.time() - handover['timestamp']) / 3600

    return json.dumps({
        "available": True,
        "timestamp": handover['timestamp'],
        "hours_ago": round(hours_ago, 1),
        "content": handover_data
    }, indent=2)
```

**Usage Pattern:**
```python
# wake_up() shows availability
wake_up()  # â†’ handover: {available: true, hours_ago: 2.5}

# Get full details only if needed
get_last_handover()  # â†’ full handover package
```

### 3. Pagination Infrastructure (NEW)

**Database Schema (claude_mcp_hybrid.py lines 513-528):**

```sql
CREATE TABLE query_results_cache (
    query_id TEXT PRIMARY KEY,           -- Deterministic hash of query
    query_type TEXT NOT NULL,            -- 'query_files', 'search_contexts', etc.
    query_params TEXT,                   -- JSON of parameters
    total_results INTEGER NOT NULL,
    result_data TEXT NOT NULL,           -- JSON array of ALL results
    created_at REAL NOT NULL,
    expires_at REAL NOT NULL,            -- TTL: 1 hour
    session_id TEXT
);
```

**Helper Functions (claude_mcp_hybrid.py lines 1202-1311):**

```python
def store_query_results(conn, query_type: str, params: dict, results: list,
                       ttl_seconds: int = 3600) -> str:
    """Store query results in database for pagination."""
    # Create deterministic query_id from query_type + params
    query_id = hashlib.md5(
        f"{query_type}:{json.dumps(params, sort_keys=True)}".encode()
    ).hexdigest()[:12]

    conn.execute("""
        INSERT OR REPLACE INTO query_results_cache
        (query_id, query_type, query_params, total_results, result_data,
         created_at, expires_at, session_id)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """, (query_id, query_type, json.dumps(params), len(results),
          json.dumps(results), time.time(), time.time() + ttl_seconds,
          get_current_session_id()))

    return query_id

def get_query_page_data(conn, query_id: str, offset: int = 0, limit: int = 20) -> dict:
    """Retrieve paginated results from stored query."""
    cursor = conn.execute("""
        SELECT query_type, query_params, total_results, result_data
        FROM query_results_cache
        WHERE query_id = ? AND expires_at > ?
    """, (query_id, time.time()))

    row = cursor.fetchone()
    if not row:
        return {"error": "Query not found or expired"}

    all_results = json.loads(row['result_data'])
    page_results = all_results[offset:offset + limit]

    return {
        "query_id": query_id,
        "query_type": row['query_type'],
        "total_results": row['total_results'],
        "offset": offset,
        "limit": limit,
        "page_size": len(page_results),
        "has_more": offset + limit < row['total_results'],
        "results": page_results
    }

def cleanup_expired_queries(conn):
    """Remove expired query results (auto-cleanup)."""
    cursor = conn.execute("""
        DELETE FROM query_results_cache WHERE expires_at < ?
    """, (time.time(),))
    deleted_count = cursor.rowcount
    if deleted_count > 0:
        conn.commit()
    return deleted_count
```

**Auto-Cleanup (claude_mcp_hybrid.py lines 1335-1336):**
```python
# In wake_up():
cleanup_expired_queries(conn)
```

### 4. get_query_page() Tool (NEW)

**Purpose:** Universal pagination for all queries

**Implementation (claude_mcp_hybrid.py lines 1841-1880):**

```python
@mcp.tool()
async def get_query_page(query_id: str, offset: int = 0, limit: int = 20) -> str:
    """
    Retrieve paginated results from a previous query.

    **Token efficiency: PREVIEW** (varies by limit, typically <5KB per page)

    Use this to retrieve more results from queries like:
    - query_files()
    - search_contexts()
    - Any future tools that return query_id for pagination

    Example:
    # Initial query returns preview + query_id
    result = query_files("authentication")
    # â†’ {total: 50, preview: [5 files], query_id: "abc123"}

    # Get next page
    page2 = get_query_page("abc123", offset=5, limit=10)
    # â†’ {results: [files 5-15], has_more: true}

    Note: Query results expire after 1 hour (TTL). If expired, run the query again.
    """
    conn = get_db()
    result = get_query_page_data(conn, query_id, offset, limit)
    return json.dumps(result, indent=2)
```

## Token Budget Classification

| Category | Budget | Use Cases | Examples |
|----------|--------|-----------|----------|
| **MINIMAL** | <500 tokens | Counts, status, availability | wake_up() context counts |
| **SUMMARY** | <2K tokens | Aggregated stats, top N | memory_analytics() |
| **PREVIEW** | <5K tokens | First page + pagination info | query_files() preview |
| **FULL** | >5K tokens | Complete content (use sparingly) | recall_context() |

## Database-First Pattern

**Standard flow for ALL tools:**

1. **Process in Python:** Do all filtering, searching, analysis
2. **Store in database:** Save full results with query_id
3. **Return summary:**
   - Counts and statistics
   - Preview (first 3-5 results)
   - query_id for pagination
4. **Lazy load:** User requests more via get_query_page()

**Example Pattern:**
```python
@mcp.tool()
async def query_files(query: str, preview_only: bool = True, limit: int = 5):
    conn = get_db()

    # 1. Process ALL files in Python
    all_matches = []  # Find all matching files

    if preview_only:
        # 2. Store full results
        query_id = store_query_results(conn, "query_files",
                                       {"query": query},
                                       all_matches)

        # 3. Return summary + preview
        return {
            "total": len(all_matches),
            "preview": all_matches[:limit],
            "query_id": query_id,
            "note": "Use get_query_page() for more results"
        }
    else:
        # Full mode (explicit request)
        return {"total": len(all_matches), "results": all_matches}
```

## Progressive Disclosure Pattern

**Three-tier access model:**

```python
# Level 1: Overview (always fast, minimal tokens)
wake_up()
# â†’ "You have 42 contexts, 5 are important, 3 are stale"

# Level 2: Explore (if user asks)
explore_context_tree(flat=True)
# â†’ List of context labels only

# Level 3: Details (if user needs specific content)
recall_context("api_spec")
# â†’ Full content of that ONE context
```

## Success Metrics

| Metric | Target | Status |
|--------|--------|--------|
| wake_up() token usage | <2KB | âœ… ~2KB |
| Context window preservation | 80%+ remaining after wake_up | âœ… ~95% |
| Common workflows | <20KB total | ðŸ”„ Testing |
| Full session capacity | Use <50% of 200K window | ðŸ”„ Testing |
| Large project support | 1000+ files efficiently | âœ… Supported |

## Files Modified

1. **claude_mcp_hybrid.py:**
   - Lines 513-528: Added query_results_cache table
   - Lines 1202-1311: Added pagination helper functions
   - Lines 1325-1385: Optimized wake_up() to return counts only
   - Lines 1791-1839: Added get_last_handover() tool
   - Lines 1841-1880: Added get_query_page() tool

2. **TOKEN_OPTIMIZATION_STRATEGY.md:** Created comprehensive guide

## âœ… Phase 2: Context Summarization (COMPLETE)

### Implemented Features

1. **âœ… recall_context()** - Added preview_only parameter
   ```python
   async def recall_context(topic: str, version: str = "latest",
                           preview_only: bool = False):
       # preview_only=True: Returns 500-char summary (~100 tokens)
       # preview_only=False: Returns full content (could be 10KB+)
       # Result: 95% token reduction in preview mode
   ```

2. **âœ… batch_recall_contexts()** - Added preview_only parameter
   ```python
   async def batch_recall_contexts(topics: str, preview_only: bool = True):
       # Defaults to summaries (prevent context overflow)
       # preview_only=True: ~100 tokens per context
       # preview_only=False: Full content for all
   ```

3. **âœ… Automatic summarization** - Already implemented in v4.1
   - `preview` column stores intelligent 500-char summaries
   - `generate_preview()` extracts key sentences and rules
   - Auto-generated on lock_context()
   - Migration support for existing contexts

### Benefits Achieved
- âœ… 95% token reduction for context preview
- âœ… Prevents context overflow when exploring multiple contexts
- âœ… Enables "scan all contexts" without filling window
- âœ… Progressive disclosure: preview â†’ full content
- âœ… Fits perfectly with database-first pattern

## Phase 3: Remaining Tool Migrations

### High Priority Tools

1. **query_files()** - Integrate pagination
   ```python
   async def query_files(query: str, preview_only: bool = True, limit: int = 5):
       # Store full results, return preview + query_id
   ```

4. **get_file_clusters()** - Return summary
   ```python
   async def get_file_clusters():
       # Return: cluster counts + top 3 files per cluster
       # Store full lists, provide query_id for each cluster
   ```

5. **scan_project_files()** - Return summary only
   ```python
   async def scan_project_files():
       # Return: statistics + warnings
       # Don't return file lists (already in database)
   ```

### Medium Priority

- explore_context_tree() - Already supports flat mode âœ…
- search_contexts() - Add pagination
- sleep() - Return summary only (store full handover)

## Documentation

- **TOKEN_OPTIMIZATION_STRATEGY.md** - Complete implementation guide
- **PHASE_2A_TOOLS.md** - Tool migration specifications
- **This file (RLM_IMPLEMENTATION_PLAN.md)** - Implementation status

---

**Status:** Phase 1 COMPLETE âœ…
- wake_up() optimized: 80-95% token reduction
- Pagination infrastructure: Ready for Phase 2
- New tools: get_last_handover(), get_query_page()

**Next:** Apply database-first pattern to remaining query tools

---

*Updated: 2025-01-25*
*Phase 1 completion: Critical token optimization implemented*
